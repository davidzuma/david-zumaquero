<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Why Do Neural Networks Work? - David Zumaquero</title>
    <meta name="description" content="Research article exploring manifold disentanglement and the mathematical foundations of neural networks by David Zumaquero.">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">
                <a href="../index.html">David Zumaquero</a>
            </div>
            <div class="nav-menu" id="nav-menu">
                <a href="../index.html" class="nav-link">Home</a>
                <a href="../blog.html" class="nav-link">Blog</a>
                <a href="../speaking.html" class="nav-link">Speaking</a>
                <a href="../contact.html" class="nav-link">Contact</a>
            </div>
            <div class="nav-toggle" id="nav-toggle">
                <span class="bar"></span>
                <span class="bar"></span>
                <span class="bar"></span>
            </div>
        </div>
    </nav>

    <!-- Header Section -->
    <section class="page-header">
        <div class="container">
            <div class="header-content">
                <h1 class="page-title">Why Do Neural Networks Work?</h1>
                <p class="page-subtitle">Exploring manifold disentanglement and the mathematical foundations of deep learning</p>
                <div class="article-meta">
                    <span class="meta-item"><i class="fas fa-user"></i> David Zumaquero</span>
                    <span class="meta-item"><i class="fas fa-tag"></i> Research</span>
                    <span class="meta-item"><i class="fas fa-calendar"></i> 2022</span>
                </div>
            </div>
        </div>
    </section>
    <!-- Article Content -->
    <main class="article-content">
        <div class="container">
            <section id="introduction">
                <h2>Introduction</h2>
                <p>
                    Neural networks excel at complex problems in computer vision, NLP, and GenAI, but are often seen as "black boxes" 
                    due to their complexity. This article explores how neural networks progressively untangle data across layers, 
                    following insights from manifold disentanglement theory.
                </p>
            </section>

            <section id="complexity-interpretability">
                <h2>The Complexity-Interpretability Trade-off</h2>
                <p>
                    Simple models like logistic regression offer interpretability but struggle with complex datasets. 
                    Neural networks provide power but sacrifice transparency. Understanding their layer-wise transformations 
                    helps bridge this gap.
                </p>
                <p>
                    For example, consider a classification task involving a dataset shaped like two interwoven spirals, where each spiral represents a different class. If you attempt to use logistic regression, you will struggle to find a function that correctly separates the two classes with a reasonable decision boundary.
                </p>
                <p>
                    Neural networks, on the other hand, are well-equipped to handle such challenges. Thanks to their multiple layers and activation functions, they can learn non-linear decision boundaries that simpler models cannot.
                </p>
                
                <div class="image-container">
                    <img src="images/Disentanglement in Neural Networks.png" alt="Logistic regression vs neural network on spirals" class="example-image">
                    <p class="image-caption">Logistic regression (left) fails to separate two interwoven spirals, while a neural network (right) succeeds.</p>
                </div>
            </div>
            <p>
                This power comes with a cost. Neural networks are often seen as black boxes due to their complexity, making it difficult to interpret the success of their results.
            </p>
        </section>

        <section id="dimensionality-reduction">
            <h2>Dimensionality Reduction</h2>
            <p>
                We tend to trust what we can see, so it would be ideal to visually inspect how data transforms as it passes through each layer of a neural network. However, most datasets exist in a high-dimensional space, often far beyond three dimensions. Similarly, the outputs of neural network layers typically have high dimensionality. We need a way to project the data into a 3D space while preserving as much information as possible. This is where dimensionality reduction techniques come into play.
            </p>
            <p>
                In this example, we apply Principal Component Analysis (PCA) to project high-dimensional data into a lower-dimensional space for visualisation.
            </p>
            <p>
                The Digits dataset consists of 1,797 images of handwritten digits, categorised into 10 classes ({0, 1, ..., 9}). For simplicity, we will focus on just two classes: 1 and 8.
            </p>
            <p>
                We train a 3-layer neural network, with 10 neurons per layer, that achieves 100% accuracy on the training set. Below, we visualise the network‚Äôs output after applying PCA in the input (training dataset) and after each layer.
            </p>
   
            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (1).png" alt="PCA of input and first layer" class="example-image">
                <p class="image-caption">PCA projection of the input data and after the first neural network layer.</p>
            </div>

            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (2).png" alt="PCA of second and third layer" class="example-image">
                <p class="image-caption">PCA projection after the second and third neural network layers.</p>
            </div>
            <p>
                One might argue that the digits originally exist in a 64-dimensional space (since each image is 8x8 pixels, flattened into a vector of 64 features), and we are reducing this space to just 3 dimensions. However, the amount of lost information is less than 5% (‚â• 95% of cumulative explained variance) for both the input and all neural network layers. To understand how PCA and explained variance work, take a look at plotly PCA.
            </p>
            <p>
                In the images, blue points represent the digit 8, while orange points represent the digit 1. As we progress through the layers, we observe a disentanglement and flattening process for the class representing 8. Additionally, points in the last layer are close to live in the same plane.
            </p>
        </section>

        <section id="measuring-entanglement">
            <h2>Measuring Entanglement</h2>
            <p>
                Measuring entanglement without relying on geometric tools seems challenging. So, instead of working with discrete points, we assume that our data lies on a manifold embedded in a m-dimensional space.
            </p>

            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (3).png" alt="Manifold example" class="example-image">
                <p class="image-caption">Example of a data manifold embedded in higher-dimensional space.</p>
            </div>
            <p>
                A way to quantify entanglement is by comparing two different distance metrics:
            </p>
            <ul>
                <li><strong>Euclidean distance</strong> ‚Äì the straight-line distance between two points in the ambient space.</li>
                <li><strong>Geodesic distance</strong> ‚Äì the shortest path between two points that remains on the manifold.</li>
            </ul>
            <p>
                In a highly entangled manifold, the difference between these two distances is significant. The Euclidean distance may suggest that two points are close, while the geodesic distance reveals that they are actually far apart when constrained to the manifold.
            </p>
            <p>
                On the other hand, in a low-entanglement manifold, the Euclidean and geodesic distances will be similar. If the manifold is a hyperplane, the two distances will be identical, indicating no entanglement.
            </p>
            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (4).png" alt="Euclidean vs geodesic distance" class="example-image">
                <p class="image-caption">Illustration of the difference between Euclidean and geodesic distances on a manifold.</p>
            </div>
        </section>

        <section id="approximating-geodesic">
            <h2>Approximating Geodesic Distance with Graphs</h2>
            <p>
                The only information we have about the manifold where our data lives are the data points themselves and we will use them to approximate the geodesic distance.
            </p>
            <p>
                The process follows these steps:
            </p>
            <ol>
                <li>Construct a connected k-nearest neighbors (k-NN) graph with k the minimum necessary to maintain a connected graph.</li>
                <li>The geodesic distance between two points is estimated as the shortest path between them within the created graph. This can be calculated using Dijkstra's algorithm.</li>
            </ol>
        </section>

        <section id="comparing-distances">
            <h2>Comparing Euclidean and Geodesic Distances</h2>
            <p>
                Now that we have a method for calculating both Euclidean distance and Geodesic distance, the next step is to compare them systematically. A simple approach would be to select two distant points and compare their distances but we need a global measure that takes into account all points in our graph.
            </p>
            <p>
                We quantify entanglement by analysing the correlation between Euclidean and geodesic distances across all data points using an adapted version of the Pearson Correlation Coefficient.
            </p>
            <p>
                The correlation coefficient, denoted as c, is defined by the following formula:
            </p>

            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (5).png" alt="Correlation coefficient formula" class="example-image">
                <p class="image-caption">Formula for the correlation coefficient used to quantify entanglement.</p>
            </div>
            <p>
                where: ùóö·¥á and ùóö·¥ç represent distances between two points, œÉ·¥á and œÉ·¥ç variances of the distances and Œº·¥á and Œº·¥ç mean; ·¥á for Euclidean distance and ·¥ç Geodesic Distance.
            </p>
            <p>
                The value of c is bounded between 0 and 2, and c close to 0 means euclidean and geodesic distances are highly correlated, nearby points in Euclidean distance remain close in geodesic distance, and distant points remain far apart. This translates that the closer c is to cero the less entanglement we find.
            </p>
            <p>
                We use the same dataset and trained neural network from the PCA example above and compute the c metric for the digit 8 across all layers of the network to run the experiment. The results are as follows:
            </p>
            <div class="image-container">
                <img src="images/Disentanglement in Neural Networks (6).png" alt="Entanglement c results" class="example-image">
                <p class="image-caption">Entanglement metric (c) results across neural network layers for digit 8.</p>
            </div>
            <p>
                This example demonstrates how neural networks progressively untangle data, making it more structured 
                and easier to classify. By comparing Euclidean and geodesic distances, we can quantify this disentanglement 
                and better understand neural network transformations.
            </p>
        </section>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-text">
                    <p>&copy; 2025 David Zumaquero. All rights reserved.</p>
                </div>
                <div class="footer-social">
                    <a href="mailto:zumaquerodavid@gmail.com" class="social-link">
                        <i class="fas fa-envelope"></i>
                    </a>
                    <a href="https://github.com/davidzuma" class="social-link">
                        <i class="fab fa-github"></i>
                    </a>
                    <a href="https://www.linkedin.com/in/davidzuma/" class="social-link">
                        <i class="fab fa-linkedin"></i>
                    </a>
                    <a href="https://x.com/zumaquerodavid" class="social-link">
                        <i class="fab fa-x-twitter"></i>
                    </a>
                </div>
            </div>
        </div>
    </footer>

    <script src="../js/main.js"></script>
</body>
</html>
